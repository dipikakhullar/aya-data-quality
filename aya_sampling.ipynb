{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cb8833d-83db-407b-8bc4-a6960df36eea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "789b339e-e304-41b9-97a4-6891624ae670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e505fc0b-cb6e-40aa-be2a-de293284ba31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_subset_names = ['aya_dataset', 'templated_afriqa', 'templated_afrisenti', 'templated_amharic_qa', 'templated_armenian_instruct', 'templated_bengali_news', 'templated_dutch_imdb', 'templated_hindi_headline', 'templated_hindi_news', 'templated_indic_paraphrase', 'templated_indic_sentiment', 'templated_indo_stories', 'templated_japanese_instruct', 'templated_joke_explaination', 'templated_ligurian_news', 'templated_masakhanews', 'templated_mintaka', 'templated_ntx_llm', 'templated_nusax_senti', 'templated_persian_farstail', 'templated_persian_instruct', 'templated_scirepeval', 'templated_seed_instruct', 'templated_soda', 'templated_tamil_stories', 'templated_tamil_thirukkural', 'templated_telugu_food', 'templated_telugu_jokes', 'templated_telugu_news', 'templated_telugu_poems', 'templated_telugu_riddles', 'templated_thai_pos', 'templated_thai_scb', 'templated_thai_usembassy', 'templated_thai_wikitionary', 'templated_turku_paraphrase', 'templated_ukranian_gec', 'templated_uner_llm', 'templated_urdu_news_category', 'templated_urdu_news_gen', 'templated_urdu_news_headline', 'templated_wiki_split', 'templated_xcsqa', 'templated_xlel_wd', 'templated_xwikis', 'translated_adversarial_qa', 'translated_cnn_dailymail', 'translated_dolly', 'translated_flan_coqa', 'translated_flan_cot', 'translated_flan_gem_wiki', 'translated_flan_lambada', 'translated_flan_qa', 'translated_hotpotqa', 'translated_joke_explaination', 'translated_mintaka', 'translated_mlqa', 'translated_nqopen', 'translated_paws', 'translated_piqa', 'translated_soda', 'translated_wiki_split', 'translated_wikiqa', 'translated_xlel_wd']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8d27d53-c746-4e6e-b9c9-d1bdd7e6def3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "templated_subsets = [s for s in all_subset_names if \"templated\" in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acf7dd33-35da-4d88-acc4-aea64a158848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "translated_subsets = [s for s in all_subset_names if \"translated\" in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e4b3a97-16c5-47e7-8d0a-0366b577e148",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translated_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d40a843-afed-4d73-aa1c-e2fc06aeb963",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263.1578947368421"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5000/19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9ab1eb5-7dce-4059-aed1-dd10b459f60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "all_subset_names = ['aya_dataset', 'templated_afriqa', 'templated_afrisenti', 'templated_amharic_qa', 'templated_armenian_instruct', 'templated_bengali_news', 'templated_dutch_imdb', 'templated_hindi_headline', 'templated_hindi_news', 'templated_indic_paraphrase', 'templated_indic_sentiment', 'templated_indo_stories', 'templated_japanese_instruct', 'templated_joke_explaination', 'templated_ligurian_news', 'templated_masakhanews', 'templated_mintaka', 'templated_ntx_llm', 'templated_nusax_senti', 'templated_persian_farstail', 'templated_persian_instruct', 'templated_scirepeval', 'templated_seed_instruct', 'templated_soda', 'templated_tamil_stories', 'templated_tamil_thirukkural', 'templated_telugu_food', 'templated_telugu_jokes', 'templated_telugu_news', 'templated_telugu_poems', 'templated_telugu_riddles', 'templated_thai_pos', 'templated_thai_scb', 'templated_thai_usembassy', 'templated_thai_wikitionary', 'templated_turku_paraphrase', 'templated_ukranian_gec', 'templated_uner_llm', 'templated_urdu_news_category', 'templated_urdu_news_gen', 'templated_urdu_news_headline', 'templated_wiki_split', 'templated_xcsqa', 'templated_xlel_wd', 'templated_xwikis', 'translated_adversarial_qa', 'translated_cnn_dailymail', 'translated_dolly', 'translated_flan_coqa', 'translated_flan_cot', 'translated_flan_gem_wiki', 'translated_flan_lambada', 'translated_flan_qa', 'translated_hotpotqa', 'translated_joke_explaination', 'translated_mintaka', 'translated_mlqa', 'translated_nqopen', 'translated_paws', 'translated_piqa', 'translated_soda', 'translated_wiki_split', 'translated_wikiqa', 'translated_xlel_wd']\n",
    "\n",
    "\n",
    "# Try to load the dataset info\n",
    "dataset = load_dataset(\"CohereForAI/aya_collection\", all_subset_names[0], split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41c3e888-1dba-42bb-b027-36891aba635e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subset: aya_dataset\n",
      "Successfully sampled 5000 entries from aya_dataset\n",
      "Saved to: aya_sampled_data/aya_dataset_train_sample.json\n",
      "\n",
      "All subsets processed.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import gc  # For explicit garbage collection\n",
    "\n",
    "def sample_from_dataset(subset_name, output_dir, num_samples=5000):\n",
    "    \"\"\"\n",
    "    Sample from a single dataset subset and save to disk.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing subset: {subset_name}\")\n",
    "    output_path = os.path.join(output_dir, f\"{subset_name}_train_sample.json\")\n",
    "    \n",
    "    try:\n",
    "        # Load only the train split\n",
    "        dataset = load_dataset(\"CohereForAI/aya_collection\", subset_name, split=\"train\")\n",
    "        \n",
    "        # Calculate indices for sampling\n",
    "        total_size = len(dataset)\n",
    "        if total_size < num_samples:\n",
    "            print(f\"Dataset has fewer than {num_samples} rows; sampling all {total_size} rows.\")\n",
    "            indices = range(total_size)\n",
    "        else:\n",
    "            indices = random.sample(range(total_size), num_samples)\n",
    "        \n",
    "        # Sample using indices\n",
    "        sampled_data = [dataset[idx] for idx in indices]\n",
    "        \n",
    "        # Save to disk\n",
    "        with open(output_path, \"w\", encoding='utf-8') as f:\n",
    "            json.dump(sampled_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"Successfully sampled {len(sampled_data)} entries from {subset_name}\")\n",
    "        print(f\"Saved to: {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {subset_name}: {str(e)}\")\n",
    "    \n",
    "    finally:\n",
    "        # Explicit cleanup\n",
    "        if 'dataset' in locals():\n",
    "            del dataset\n",
    "        if 'sampled_data' in locals():\n",
    "            del sampled_data\n",
    "        gc.collect()  # Force garbage collection\n",
    "        \n",
    "# Create output directory\n",
    "output_dir = \"translated_subsets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process one subset at a time\n",
    "# templated_subsets = [\"aya_dataset\"]\n",
    "for subset_name in translated_subsets:\n",
    "    sample_from_dataset(subset_name, output_dir)\n",
    "\n",
    "print(\"\\nAll subsets processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63e210c9-6cf4-4a30-a1fa-01b037283764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Directory containing the sampled JSON files\n",
    "input_dir = \"aya_sampled_data\"\n",
    "\n",
    "# List to store data from all JSONs\n",
    "all_data = []\n",
    "\n",
    "# Iterate through all JSON files in the directory\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        subset_name = file_name.rsplit(\".\", 1)[0]  # Get the file name without the extension\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        \n",
    "        # Load the JSON file\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        # Add a column for subset_name\n",
    "        for entry in data:\n",
    "            entry[\"subset_name\"] = subset_name\n",
    "        \n",
    "        # Append to the all_data list\n",
    "        all_data.extend(data)\n",
    "\n",
    "# Create a pandas DataFrame from the combined data\n",
    "df = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "926624f9-4c6d-482d-888e-ae24759b4597",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully saved to aya_sample_5k.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to a Parquet file without the index\n",
    "output_file = \"aya_sample_5k.parquet\"\n",
    "df.to_parquet(output_file, index=False)\n",
    "\n",
    "print(f\"DataFrame successfully saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "950b7c6e-3e35-4ddb-a1d9-42c32ab855b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76fc57b-6404-411e-b187-fb8db56882fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
